{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nearest neighbor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this can either classification or regression\n",
    "\n",
    "for k-neighbors classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "the most important varible is the n_neighbors\n",
    "\n",
    "for k_neighbors regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "the most important parameter is the n_neighbors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models for regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear models for regression\n",
    "1. LinearRegression aka Ordinary Least Squares\n",
    "simples and the most classic linear method for regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "2. Ridge regression is also a linear model for regression \n",
    "if we are interested in generalization performance, we should choose the ridge model over the linearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "The ridge model makes a trade-off between the simplicity of the model (near zero coefficients) and its perfromance on the training set. How much more importance the model places on simplicity versus training set perfromance can be specified by the user, using the alpha parameter. \n",
    "THE MOST IMPORTANT PARAMETER IS THE ALPHA PARAMETER\n",
    "\n",
    "3. Lasso : it is an alternative to ridge for regularizing linear regression. The lasso also restricts coefficients to be close to zero, similarly to ridge regression, but in a slightly different way, called the 'l1' regularization.\n",
    "\n",
    "The consequence of the l1 regularization is that when using the Lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero offen makes a model easier to interpret, and can reveal the most important features of your model.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "THE MOST IMPORTANT PARAMTER IS THE ALPHA PARAMETER JUST AS THE RIDGE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model for classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For linear models for classification, the decision boundary is a linear function of the input. In other words, a binary classifier (linear) is a classifier that separates two classes using a line, a plane, or a hyperplane.\n",
    "\n",
    "1. LogisticRegression\n",
    "2. linear support vector machines (linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classifier).\n",
    "\n",
    "from sklearn.liner_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "for LogisticRegression and LinearSVC the trade-off paramter that dertermines the strength of the regularization is called C, and higher values of C correspond to less regularization. In other words, when using a high value of the parameter C, logisticRegression and LinearSVC try to fit the training set as best as possible, while when with low values of the parameter C, the model put more emphasis on finding a coefficient vector w that is close to zero.\n",
    "There is another interesting intuition of how the paramter C acts. Using low values of C will cause the algorithms to try to adjust to the \"majority\" of data points, while using a higher value of C stresses the importance that each individual data point be classified correctly.\n",
    "Logistic regression applies to L2 regularization by default.\n",
    "\n",
    "That is important parameter C and penalty = 'L1' or 'L2'\n",
    "L1 regularization gives a more interpretable model as it limits the model to only using a few features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear models regression and classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The main parameter of linear models is the regularization parameter, called alpha in the  regression models and  C in linearSVC and LogisticRegression. Large alpha or small C means simple models.\n",
    "\n",
    "The other decision you have to make is wheather you want to use the L1 regualarization or L2 regularization. If you assume that only few of your features are acutally important, you should use L1. Otherwise, you should default to L2.\n",
    "L1 can be used if interpretability of the model is important. As L1 will only use a few features, it is easier to explain which features are important to the model and what the effect of these features are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models are very fast to train, and also fast to predict. They scale to very large datasets and work well with sparse data. If your data consists of hunderds of thousands or millions of samples, you might want to investigate SGDClassifier and SGDRegressor, which implement even more version  of the linear model described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed above. However, they tend to be even faster in training. The price paid is for efficiency is that naive Bayes model often provide genearalization perfromance that is slightly worse than linear classifiers like logisticRegression and LinearSVC.\n",
    "\n",
    "There are three kinds of naive Bayes classifiers implemeted in scikit-learn, GaussianNB, BernoulliNB, and MultinomialNB.\n",
    "\n",
    "GaussianNB can be applied to any continuous data, while BernoulliNB assumes binary data and MultinomialNB assumes cound data (that is each feature represents an integer count of something, like how often a word appears in a sentence. BernoulliNB and MultinomialNB are mostl used in text data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# strength, weakness and parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The MultinomialNB and BernoulliNB have a single parameter alpha, which controls model complexity. The way alpha works is that the algorithm adds alpha many virtual data points to the data, that have positive valuess for all the features. This results in a \"smoothening\" of the statistics. A large alpha means more smoothing, resulting in less complex models. The algorithms perfromance is relatively robust to the setting of alpah, meaning that setting alpha is not critical for good performance However, tuning it usually improves accuracy somewhat.\n",
    "\n",
    "The GaussianNB model seems to be rarely used by practitioners.\n",
    "\n",
    "MultinomialNB usually perfroms better than BinaryNB, in particuar on datasetw with a relatively large number of non-zero features (i.e larger documents).\n",
    "\n",
    "The Naive Bayes are fast tor train and predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data, and are relatively robust to the parameters. Naive Bayes models are great baseline models and are often used on very large datasets, where training even a lienar model might take too long. Naive Bayes share many of the strengths and weakness of the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees regression and classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The controlling parameter is max_depth.\n",
    "increase in the paramter depth increases regularization or improved model.\n",
    "\n",
    "This algorithm is based on the if-else qu3estion.\n",
    "\n",
    "There are two common strategies to prevent overfitting stopping the creation of the tree early, also called pre-prunning, or building the tree but then removing or collapsing nodes that contain little information, also called post-prunning or just prunning.\n",
    "\n",
    "Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and\n",
    "DecisionTreeClassifier classes. Scikit-learn only implements pre-prunning and not post-prunnings.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "limiting the depth of the tree decrease overfitting.\n",
    "\n",
    "We can visualize the tree using the export_graphviz function from the tree module.\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "or use import graphviz to read the dot file\n",
    "\n",
    "FEATURE IMPORTANCE IN TREES\n",
    "the feature importance rates how important each feature is for the decision tree makes. It is a number between 0 and 1 for each feature, where 0 means \"not used at all\" and 1 means \"perfectly predicts the target\". \n",
    "The feature importances always sum to one.\n",
    "\n",
    "you access feature importance with command:\n",
    "\n",
    "tree.feature_importances_\n",
    "you can visualize the result by plotting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# strength, weakness and parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The parameter that controls model complexity in decision trees are the pre-prunning parameters that stop the building of the tree before it is fully developed. Usually picking one of the pre-prunning strategies, either setting min_depth, max_leaf_nodes or min_samples_leaf is to prevent overfitting.\n",
    "\n",
    "\n",
    "The advantage of decision tree is that the resulting model can easily be visualized and understood by non-experts(at least for smaller trees), and the algorithms is completely invariant to scaling of the data: As each feature is processed separately, and the possible splits of the data don't depend on scaling, no preprocessing like normalization or standardization or features is needed for decision tree algorithms.\n",
    "\n",
    "decision tree works well when you have features that are on completely different scales, or a mix of binary  and continuous features.\n",
    "\n",
    "The main down side of decision trees is that even with the use of pre-prunning decision trees tend to overfit, and provide poor genralization perfromance.\n",
    "\n",
    "Therefore, in most applications, the ensemble methods discussed below are usually used in place of a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles of Decision Trees"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ensembles are methods that combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "Random Forests and Gradient Boosted Decision Trees\n",
    "\n",
    "Random Forests:\n",
    "As observed above, a main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. Random forests are essentially a collection of decision trees where each tree is slightly different from the other.\n",
    "\n",
    "Building Random Forests:\n",
    "To build a random forest model, you need to decide on the number of trees to build (the n_estimator parameter of RandomForestRegressor or RandomForestClassifier). Lets say we want to build ten trees. These trees will be built completely independent from each other and will make random choices to make sure they are distinct [the trees make random choice].\n",
    "To build a tree, we first take what is called a bootstrap sample of our data.\n",
    "\n",
    "A bootstrap sample means from our n_samples data ponts, we repeatedly draw an example randomly with replacement (i.e the same sample can be picked multiple times ), n_samples times. This will create a dataset that is big as the original dataset but some data points will be missing from it, and some will be repeated.\n",
    "\n",
    "Instead of looking for the best test for each node, in each node the algorithm randomly selects a subset of the features, and looks for the best possible test involving one of these features. The amount of features that is selected is controlled by the MAX_FEATURE PARAMETER.\n",
    "\n",
    "IMPORTANT PARAMETERS ARE THE N-SAMPLES or n_estimators, random_state AND MAX-FEATURE PARAMETER.\n",
    "\n",
    "A critical parameter in this process is max_features. If we set max_features to n_features, that means that each split can look at all features in the dataset, and no randomness will be injected. if we set max_features to one, that means that the splits have no choice at all on which feature to test and call only search over differnet thresholds for the feature that was selected randomly.\n",
    "\n",
    "Therefore, a high max_feature means that the tree in the random forest will be quite similar, and they will be able to fit the data easily, using the most distinctive features. A low max_features means that the trees in the random forest will be quite different and that each tree might need to be very deep in order to fit the data well.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "YOU CAN ADJUST THE NUMBER OF CORES TO USE WITH THE n_jobs parameter. USING MORE CPU CORES WILL RESULT IN LINEAR SPEED UPS, BUT SPECIFYING n_jobs larger than the number of cores will not help. you can set n_jobs = -1 to use all the cores in your computer.\n",
    "\n",
    "Random forest don't tend to perfrom well on high dimensional, sparse data such as text data. For this kind of data, linear models might be more appropriate.\n",
    "Random forest usually works weel on very large datasets, and training can easily be parallelized over many CPU cores within a powerful computer. However, random forests require more memeory and are slower to train and to predict than linear models. If time and memory are important in an application, it might make sense to use a linear model instead.\n",
    "\n",
    "\n",
    "The important parameters to adjust are n_estimators, max_features and possibly pre_prunning options like max_depth. For n_estimators, larger is always better.\n",
    "max_features determines how random each tree is, and a smaller max_features reduces overfitting. The default values, and a good rule of thumb, are max_features = sqrt(n_features) for classification and max_features = log2(n_features) for regression.\n",
    "\n",
    "adding max_features or max_leaf_nodes might sometimes improve perfromance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient Boosted Regression Trees(Gradient Boosting Machines)\n",
    "\n",
    "It is an essemble method that can be used for both regression and classification tasks.\n",
    "\n",
    "In contrast to random forest, gradient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one. There is no randomization in gradient boosted regression trees; instead, strong pre-prunning is used.\n",
    "\n",
    "Apart from the pre-prunning and the number of trees in the ensemble, another important parameter of gradient boosting is the learning_rate which controls how strongly each tree tries to correct the mistakes of the previous trees. A higher learning rate means each tree can make stronger corrections, allowing for more complex models. Similarly, adding more trees to the ensemble, which can be done by increasing n_estimators, also increases the model complexity, as the model has more chances to correct mistakes on the training set.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "IMPORTANT PARAMETERS ARE PRE-PRUNNING, LEARNING_RATE, MAX_DEPTH...\n",
    "\n",
    "As gradient boosting and random forest perform well on similar kinds of data, a common approach is to first try random forests, which work quite robustly. If random forest work well, but prediction time is at a premium, or it is important to squeeze out the last percentage of accuracy from the machine learning model, moving to gradient boosting often helps.\n",
    "\n",
    "If you want to apply gradient boosting to a larger scale problem, it might be worth looking into the xgboost package and its python interface, which at the time of writing is faster (and sometimes easier to tune) that the sklearn implementation of gradient boosting on many datasets.\n",
    "\n",
    "Their main drawback is that they require careful tuning of the parameters, and may take long time to train.\n",
    "\n",
    "The main parameters of the gradient boosted tree models are the number of trees n_estimators, and the learning_rate, which controls how much each tree is allowed to correct the mistakes of the previous trees.\n",
    "\n",
    "Lower learning_rate means that more trees are nedded to build a model of similar complexity. In contrast to random forests, where higher n_estimators is always better, increasing n_estimators in gradient boosting leads to a more complex model, which may lead to overfitting.\n",
    "\n",
    "A common practice is to fit n_estimators depending on time and memory budget, and then search over different learning_rates. \n",
    "Another important parameter is max_depth, which is usually very low for gradient boosted models, often not deeper than five splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernelized support vector machines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Kernelized SVM is an extension of linear SVM that allows for more complex models which are not defined simply  by hyperplanes in the input space.\n",
    "\n",
    "\n",
    "To tune SVM parameters, the gamma and C paramters are the paramter tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Deep learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Neural Network model\n",
    "MLPs( multilayer perceptrons for classification and regression\n",
    "\n",
    "MLPs can be viewed as genearalizations of linear models which perform multiple states of processing to come to a decision.\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "tunning paramters are algorithm = 'l-bfgs',hidden_layer_sizes (to reduce model complexity), \n",
    "\n",
    "with no of hidden units if decision boundary look....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
